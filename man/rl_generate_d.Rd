% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/rl_generate_d.R
\name{rl_generate_d}
\alias{rl_generate_d}
\title{rl_generate_d}
\usage{
rl_generate_d(
  data,
  L_choice,
  R_choice,
  L_reward,
  R_reward,
  time_line,
  sub_choose,
  var1 = NA,
  var2 = NA,
  initial_value = NA,
  softmax = TRUE,
  seed = 123,
  eta,
  beta = 1,
  tau,
  epsilon,
  lambda = NA,
  beta_func,
  eta_func,
  prob_func,
  digits = 2
)
}
\arguments{
\item{data}{Data for each subject}

\item{L_choice}{The column name for the left option}

\item{R_choice}{The column name for the right option}

\item{L_reward}{The column name for the reward of right option}

\item{R_reward}{The column name for the reward of left option}

\item{time_line}{Variables used to represent the experimental timeline, such as block and trial}

\item{sub_choose}{subject choose}

\item{var1}{extra variable 1}

\item{var2}{extra variable 2}

\item{initial_value}{The initial value you assign to a stimulus, defaulting to 0}

\item{softmax}{use softmax or not, defaulting to TRUE}

\item{seed}{seed}

\item{eta}{In the RSTD model, the learning rate is different for positive and negative conditions.}

\item{beta}{In the Utility model, it is assumed that all rewards will be discounted}

\item{tau}{The τ parameter in the soft-max function, with a default value of 1}

\item{epsilon}{In the WXT model, the discount rate is divided into different intervals.}

\item{lambda}{Other parameters that you think might influence the softmax function}

\item{beta_func}{The function for the discount rate β, which you can customize}

\item{eta_func}{The function for the learning rate η, which you can customize}

\item{prob_func}{The soft-max function, which you can customize.}

\item{digits}{digits}
}
\value{
generated data
}
\description{
rl_generate_d
}
